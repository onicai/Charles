{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify we're in the Conda environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "print(sys.executable)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import python packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import openai\n",
                "from PIL import Image\n",
                "import base64\n",
                "import io\n",
                "from dotenv import load_dotenv\n",
                "import requests\n",
                "from openai import OpenAI\n",
                "import pprint\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import subprocess\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## openAI API key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up your OpenAI API key\n",
                "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
                "\n",
                "# Load the .env file\n",
                "load_dotenv()\n",
                "\n",
                "client = OpenAI(\n",
                "    # This is the default and can be omitted\n",
                "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Helper functions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function to base64 encode an image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def encode_image(image_path):\n",
                "    \"\"\"Encode the image to base64 format to send to OpenAI.\"\"\"\n",
                "    with open(image_path, \"rb\") as image_file:\n",
                "        return base64.b64encode(image_file.read()).decode('utf-8')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function to load existing results from JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_existing_results(filename):\n",
                "    \"\"\"Load existing data from JSON file if it exists.\"\"\"\n",
                "    if os.path.exists(filename):\n",
                "        with open(filename, 'r') as f:\n",
                "            return json.load(f)\n",
                "    return []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function to send the request to OpenAI API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to send the request to OpenAI API\n",
                "def get_image_description_and_prompts(prompt, base64_image):\n",
                "    chat_completion = client.chat.completions.create(\n",
                "        messages=[\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": [\n",
                "                    {\n",
                "                        \"type\": \"text\",\n",
                "                        \"text\": prompt\n",
                "                    },\n",
                "                    {\n",
                "                        \"type\": \"image_url\",\n",
                "                        \"image_url\": {\n",
                "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
                "                        }\n",
                "                    }\n",
                "                ]\n",
                "            }\n",
                "        ],\n",
                "        model=\"gpt-4o-mini\",\n",
                "        response_format={\"type\": \"json_object\"},\n",
                "        max_tokens=10000\n",
                "    )\n",
                "\n",
                "    if (chat_completion.choices[0].finish_reason != \"stop\"):\n",
                "        print(\"Something went wrong during openAI call - finish_reason is not 'stop' \")\n",
                "        print(chat_completion)\n",
                "        sys.exit(1)\n",
                "\n",
                "    content = chat_completion.choices[0].message.content.strip()\n",
                "\n",
                "    # Convert the content to a dictionary\n",
                "    try:\n",
                "        data = json.loads(content)\n",
                "    except json.JSONDecodeError:\n",
                "        print(\"Error: The content is not valid JSON.\")\n",
                "        sys.exit()\n",
                "\n",
                "    pprint.pprint(data)\n",
                "    return data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function to call llama2.c"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_story(opening_sentence):\n",
                "    # Define the command as a list\n",
                "    command = [\n",
                "        \"../..//llama2.c/run\",\n",
                "        \"../models/out-09/model.bin\",\n",
                "        \"-z\", \"../models/out-09/tok4096.bin\",\n",
                "        \"-t\", \"0.1\",\n",
                "        \"-p\", \"0.9\",\n",
                "        \"-i\", opening_sentence\n",
                "    ]\n",
                "\n",
                "    # Run the command and capture the output\n",
                "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
                "\n",
                "    # Process the output to exclude the \"achieved tok/s\" line\n",
                "    output_lines = result.stdout.strip().split('\\n')\n",
                "    story = \"\\n\".join(line for line in output_lines if \"achieved tok/s\" not in line)\n",
                "\n",
                "    return story"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function to process the images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to process N images\n",
                "def process_images_with_llama2(num_stories_with_llama2, output_file):\n",
                "    results = []\n",
                "\n",
                "    # If output file exists, load existing data\n",
                "    if os.path.exists(output_file):\n",
                "        with open(output_file, 'r') as f:\n",
                "            try:\n",
                "                results = json.load(f)\n",
                "            except json.JSONDecodeError:\n",
                "                print(f\"Warning: Could not parse existing data in {output_file}. Starting fresh.\")\n",
                "    \n",
                "    # Create a set of image paths already processed\n",
                "    existing_image_paths = {entry[\"image\"] for entry in results}\n",
                "\n",
                "    # Base directory containing all the folders\n",
                "    base_dir = Path(\"/Scandisk/onicai/charles/images\")\n",
                "    image_paths = [path for path in base_dir.glob(\"**/*.png\") if not path.name.startswith(\".\")]\n",
                "    thumbnail_size = (100, 100)\n",
                "\n",
                "    icount = 0\n",
                "    for image_path in image_paths:\n",
                "        icount += 1\n",
                "        print(f\"------------------\\n Processing image {icount}: {image_path}\")\n",
                "        # # Open and display the image\n",
                "        image = Image.open(image_path)\n",
                "        image.thumbnail(thumbnail_size)  # Resize the image to a thumbnail\n",
                "        plt.figure(figsize=(2, 2))  # Adjust figure size\n",
                "        plt.imshow(image)\n",
                "        plt.axis('off')  # Hide axes for better view\n",
                "        plt.show()\n",
                "\n",
                "        # Make sure this image has already been processed by openAI\n",
                "        if not (str(image_path) in existing_image_paths):\n",
                "            print(\"This image was not yet processed. No opening_sentences found. Skipping...\")\n",
                "            continue\n",
                "\n",
                "        existing_entry = next(entry for entry in results if entry[\"image\"] == str(image_path))\n",
                "        \n",
                "        # check if we already generated llama2.c stories for this entry\n",
                "        if \"opening_sentences_with_stories\" in existing_entry:\n",
                "            print(f\"Already done, found opening_sentences_with_stories for this image\")\n",
                "            continue\n",
                "        \n",
                "        # Extract data (format is a little strange, but that's what it is...)\n",
                "        description = existing_entry[\"response\"][\"response\"][\"description\"]\n",
                "        opening_sentences = existing_entry[\"response\"][\"response\"][\"opening_sentences\"]\n",
                "\n",
                "        print(f\"description = {description}\")\n",
                "\n",
                "        # Loop over all opening_sentences, and for each opening_sentence, create 3 stories\n",
                "        # Then add these storeis back into the results dictionary\n",
                "        response_entry = {\n",
                "            \"image\": str(image_path),\n",
                "            \"description\": description,\n",
                "            \"opening_sentences_with_stories\": []\n",
                "        }\n",
                "        for i,sentence in enumerate(opening_sentences):\n",
                "            story_set = []\n",
                "            for ii in range(num_stories_with_llama2):\n",
                "                print(f\"For opening_sentence {i}, calling llama2.c to generate story variant {ii}\")\n",
                "                story = generate_story(sentence)\n",
                "                story_set.append(story)\n",
                "            \n",
                "            # Append the generated stories under each opening_sentence\n",
                "            response_entry[\"opening_sentences_with_stories\"].append({\n",
                "                \"opening_sentence\": sentence,\n",
                "                \"story_set\": story_set\n",
                "            })\n",
                "\n",
                "            # pprint.pprint(response_entry)\n",
                "\n",
                "        # Find the index of the existing entry and replace it\n",
                "        entry_index = next((i for i, entry in enumerate(results) if entry[\"image\"] == str(image_path)), None)\n",
                "        if entry_index is not None:\n",
                "            print(f\"Replacing entry {entry_index} in results\")\n",
                "            results[entry_index] = response_entry  # Replace the existing entry\n",
                "        else:\n",
                "            # This should never happen, but keep it as a fallback to append\n",
                "            print(\"Strange... How can we not find the index ??? Please investigate...\")\n",
                "            results.append(response_entry)  \n",
                "    \n",
                "        # Save the updated results back to the output file\n",
                "        print(f\"Overwriting {output_file} with updated results\")\n",
                "        with open(output_file, 'w') as f:\n",
                "            json.dump(results, f, indent=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_file = './2-stories-by-llama2.json'\n",
                "num_stories_with_llama2 = 2  # For each example_sentence, try num stories with llama2.c\n",
                "\n",
                "process_images_with_llama2(num_stories_with_llama2, output_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "btc-stories",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
